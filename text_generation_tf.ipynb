{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/1sDk3L4es4dchfhyXLiw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bellomusodiq/machine_learning/blob/master/text_generation_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ovq43Ox6pnk",
        "colab_type": "code",
        "outputId": "9b48aeb2-5b16-41d4-98e8-2a761c6ae0ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import os \n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn7HzXkd8lQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQgEwH_d8_5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# path_to_file = list(files.upload().keys())[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYl7BB_E9Lgg",
        "colab_type": "code",
        "outputId": "1a8d8024-a175-4498-98df-a6e6a2945515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print('lenght of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lenght of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0YHNGFB9rR0",
        "colab_type": "code",
        "outputId": "9ceadcff-3dfd-42cd-c4bf-e73f6daf5c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euug204I9uMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oExYMAn-Zbl",
        "colab_type": "code",
        "outputId": "a2964628-20a1-400c-8a8d-65480cf29f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(\"Text:\", text[:20])\n",
        "print(\"Encoded:\", text_to_int(text[:20]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: First Citizen:\n",
            "Befor\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1gHrxUc-26Q",
        "colab_type": "code",
        "outputId": "c226acfc-910b-4884-b4b3-3cb430594733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "int_to_text([35,54,6,22])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Wp,J'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh5eUbe4iPPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length+1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNCfoTyXi6uf",
        "colab_type": "code",
        "outputId": "0ec521cb-87eb-4104-b446-498ac98af56f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "for data in char_dataset.take(13):\n",
        "  print(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(18, shape=(), dtype=int64)\n",
            "tf.Tensor(47, shape=(), dtype=int64)\n",
            "tf.Tensor(56, shape=(), dtype=int64)\n",
            "tf.Tensor(57, shape=(), dtype=int64)\n",
            "tf.Tensor(58, shape=(), dtype=int64)\n",
            "tf.Tensor(1, shape=(), dtype=int64)\n",
            "tf.Tensor(15, shape=(), dtype=int64)\n",
            "tf.Tensor(47, shape=(), dtype=int64)\n",
            "tf.Tensor(58, shape=(), dtype=int64)\n",
            "tf.Tensor(47, shape=(), dtype=int64)\n",
            "tf.Tensor(64, shape=(), dtype=int64)\n",
            "tf.Tensor(43, shape=(), dtype=int64)\n",
            "tf.Tensor(52, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxjaKr17i-7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyQL0PrZjeC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1] # hell\n",
        "  target_text = chunk[1:] # ello\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-UPtKiskQze",
        "colab_type": "code",
        "outputId": "3e6bf3ec-9b00-4667-882d-d667e1288175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        }
      },
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print('Input')\n",
        "  print(int_to_text(x.numpy()))\n",
        "  print('Target')\n",
        "  print(int_to_text(y.numpy()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "Target\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "Input\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "Target\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HwaM-74lNYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1So32gqlk0C",
        "colab_type": "code",
        "outputId": "da0b2375-4f1b-4529-8b54-99e2125f535a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                         return_sequences=True,\n",
        "                         stateful=True,\n",
        "                         recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71u5E3wDpEFY",
        "colab_type": "code",
        "outputId": "c7a3746f-11ca-4ef8-915f-15bbf7e144bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating Loss\n",
        "for input, target in data.take(1):\n",
        "  example_batch_predictions = model(input)\n",
        "  print(example_batch_predictions.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkfQamtzqvmU",
        "colab_type": "code",
        "outputId": "2c6ef9cf-ddef-4c0d-dc23-a7bb0a99bf8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 2.9944372e-03 -4.5068813e-03 -3.1000827e-03 ... -3.1774025e-04\n",
            "   -1.6128606e-03  1.1124160e-03]\n",
            "  [ 1.1216357e-03 -5.1360452e-03 -3.8138372e-03 ...  1.2319759e-03\n",
            "   -4.6296790e-03  1.8391861e-03]\n",
            "  [ 1.6608776e-03 -6.5122126e-03 -2.0206091e-04 ... -1.5949495e-03\n",
            "   -8.4232176e-03  1.9416037e-03]\n",
            "  ...\n",
            "  [ 1.0853268e-02 -1.2408068e-02  6.3565923e-03 ... -5.0725862e-03\n",
            "   -9.2509948e-04 -7.0947437e-03]\n",
            "  [ 4.2753620e-03 -1.0337851e-02  1.0707083e-03 ... -9.3835695e-03\n",
            "    3.8445895e-03 -5.9293816e-03]\n",
            "  [ 5.5078398e-03 -1.1233082e-02  3.1831919e-03 ... -6.0729212e-03\n",
            "   -3.2890388e-03 -4.4029793e-03]]\n",
            "\n",
            " [[ 3.4486677e-03 -1.6382255e-03 -8.9859311e-04 ...  1.8297890e-03\n",
            "    3.1552776e-03 -1.8000109e-03]\n",
            "  [-9.7283430e-04 -1.7215237e-03 -3.5312944e-03 ... -4.4550165e-03\n",
            "    7.8550149e-03 -1.7217596e-03]\n",
            "  [-1.6082181e-03 -3.8894424e-03 -1.8241321e-03 ... -4.2194356e-03\n",
            "    1.2896698e-02 -1.9532838e-03]\n",
            "  ...\n",
            "  [ 4.6825632e-03 -4.3716710e-03  5.6542465e-03 ... -8.3767613e-03\n",
            "    7.4291988e-03  4.8989779e-04]\n",
            "  [-6.7233341e-06 -3.8116339e-03  7.5336304e-03 ... -9.0082558e-03\n",
            "    7.4647316e-03 -2.3545809e-03]\n",
            "  [ 2.2038470e-03 -4.5762961e-03  9.2081837e-03 ... -5.3430591e-03\n",
            "    6.0253951e-04 -7.9068763e-04]]\n",
            "\n",
            " [[-1.7373480e-03  5.9174723e-04  4.8484635e-03 ... -1.5889851e-03\n",
            "    7.2752312e-04 -2.8074216e-03]\n",
            "  [ 3.6702259e-03 -3.4543073e-03  1.1032660e-02 ... -4.5518037e-03\n",
            "    4.2693298e-03 -5.9160776e-03]\n",
            "  [ 5.4971045e-03  5.1459926e-04  9.6790958e-03 ... -6.5223966e-04\n",
            "   -1.7226773e-03 -6.3064699e-03]\n",
            "  ...\n",
            "  [-5.3240359e-03 -4.7749979e-03 -1.1035401e-03 ... -1.1491056e-02\n",
            "   -1.7896974e-03 -5.3659822e-03]\n",
            "  [ 7.5297884e-04 -7.6543125e-03  5.3495183e-03 ... -1.2847674e-02\n",
            "    1.8540351e-03 -8.9575462e-03]\n",
            "  [ 2.2332225e-04 -4.8428643e-03 -2.4196296e-03 ... -1.2184210e-02\n",
            "    1.6112309e-03 -6.6646761e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 5.0240452e-03 -4.2190999e-03  7.0846984e-03 ... -2.9824262e-03\n",
            "    3.4831462e-03 -3.8370057e-03]\n",
            "  [ 6.7058774e-03 -2.7623051e-04  6.4364690e-03 ...  8.0519984e-04\n",
            "   -2.5217654e-03 -4.8970757e-03]\n",
            "  [ 3.9364207e-03 -2.5314430e-03  6.1950181e-03 ...  5.1532348e-04\n",
            "    5.0199544e-03 -5.3440812e-03]\n",
            "  ...\n",
            "  [ 9.4982944e-03 -8.1200441e-03  5.2940943e-03 ... -7.4464567e-03\n",
            "    5.5036205e-04 -4.5078178e-04]\n",
            "  [ 2.2630566e-03 -9.0694763e-03  6.1132880e-03 ... -8.2684234e-03\n",
            "    3.1973412e-03 -3.8148013e-03]\n",
            "  [-7.6496485e-04 -9.5580807e-03  4.9155499e-03 ... -6.5077213e-03\n",
            "    1.0714175e-02 -3.4197839e-03]]\n",
            "\n",
            " [[ 3.2577068e-03 -1.0223152e-03 -1.7498719e-04 ... -1.2536773e-03\n",
            "   -8.3160847e-03  6.7668618e-03]\n",
            "  [ 5.2216314e-03 -4.8172092e-03 -2.9510569e-03 ... -9.1909175e-04\n",
            "   -7.1060574e-03  5.4488564e-03]\n",
            "  [ 6.3465815e-03 -6.4712125e-03  1.3659346e-03 ...  3.6112173e-04\n",
            "   -1.0127356e-02  3.5518762e-03]\n",
            "  ...\n",
            "  [ 3.5909098e-04  6.1908085e-04 -1.9174350e-03 ... -4.0779049e-03\n",
            "    4.5834794e-03 -4.2064618e-03]\n",
            "  [ 5.3874478e-03 -3.0148095e-03  5.5637765e-03 ... -6.1154254e-03\n",
            "    6.1052749e-03 -8.1995083e-03]\n",
            "  [ 9.0059228e-03 -6.2780408e-03  1.1046024e-02 ... -7.4931905e-03\n",
            "    7.4208602e-03 -1.1092546e-02]]\n",
            "\n",
            " [[-5.1117823e-03 -2.6125312e-03  2.7148202e-03 ... -1.8212800e-03\n",
            "    4.2341705e-03 -4.6631936e-03]\n",
            "  [-9.8015238e-03 -8.1964508e-03  2.6229075e-03 ... -1.1403162e-03\n",
            "    2.4837640e-03  3.2857088e-03]\n",
            "  [-7.6889941e-03 -2.4393294e-03  3.2481807e-03 ... -8.0175645e-04\n",
            "    1.5489185e-03 -3.1411485e-04]\n",
            "  ...\n",
            "  [-4.0093148e-03 -1.2214820e-02  4.8382678e-03 ... -9.1869440e-03\n",
            "   -1.0513200e-03  1.7474500e-03]\n",
            "  [-3.8539716e-03 -1.0482505e-02  2.9523405e-03 ... -9.9454531e-03\n",
            "   -4.7139404e-03 -1.2226705e-03]\n",
            "  [-8.3520133e-03 -9.1186110e-03 -2.1318444e-03 ... -1.2727058e-02\n",
            "    7.3111849e-04 -1.2068925e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHqs3Ua8q_7K",
        "colab_type": "code",
        "outputId": "2d53e729-dfe8-4b2f-d713-79d6fbf045c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 0.00299444 -0.00450688 -0.00310008 ... -0.00031774 -0.00161286\n",
            "   0.00111242]\n",
            " [ 0.00112164 -0.00513605 -0.00381384 ...  0.00123198 -0.00462968\n",
            "   0.00183919]\n",
            " [ 0.00166088 -0.00651221 -0.00020206 ... -0.00159495 -0.00842322\n",
            "   0.0019416 ]\n",
            " ...\n",
            " [ 0.01085327 -0.01240807  0.00635659 ... -0.00507259 -0.0009251\n",
            "  -0.00709474]\n",
            " [ 0.00427536 -0.01033785  0.00107071 ... -0.00938357  0.00384459\n",
            "  -0.00592938]\n",
            " [ 0.00550784 -0.01123308  0.00318319 ... -0.00607292 -0.00328904\n",
            "  -0.00440298]], shape=(100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t96CB7NmrHrJ",
        "colab_type": "code",
        "outputId": "224d55e3-df5e-4289-b99d-438330631af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 2.9944372e-03 -4.5068813e-03 -3.1000827e-03 -7.7587593e-04\n",
            " -3.4814174e-03 -5.3836085e-04  7.1866480e-03  5.9816323e-04\n",
            " -2.2754211e-03  4.0485733e-03 -2.3087640e-03  2.5940430e-03\n",
            "  3.1151713e-03 -3.8202509e-04  1.6324642e-03 -1.4141508e-03\n",
            " -1.5222550e-03  1.3013456e-03  3.0231890e-03  6.2615396e-03\n",
            " -3.4406469e-03 -3.3368871e-03  2.2406637e-04  5.6786863e-03\n",
            "  9.5779949e-05 -3.6917701e-03 -1.3519300e-03  9.3230035e-04\n",
            " -1.9535606e-03 -2.6999391e-04  1.2188258e-03 -8.7019498e-04\n",
            " -3.3125200e-04  2.8198338e-03 -1.3978061e-03 -5.1671248e-03\n",
            "  1.2175059e-03 -3.3976950e-03 -3.7551245e-03  1.2030629e-03\n",
            " -5.8415718e-03  2.3232882e-03  3.6471770e-03  4.1435822e-03\n",
            "  2.2625877e-03  9.8045953e-03 -1.5206988e-03 -1.6148288e-03\n",
            " -5.3044409e-03 -1.1012729e-03 -7.6781888e-04 -1.7113908e-04\n",
            "  3.6996393e-03  2.4976657e-04 -1.2880901e-03  2.5610146e-03\n",
            "  4.9839900e-03  3.6135672e-03  2.1634914e-03  4.5661489e-03\n",
            "  1.1059704e-03 -3.0801084e-04 -3.1774025e-04 -1.6128606e-03\n",
            "  1.1124160e-03], shape=(65,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y5YJnIqrVyD",
        "colab_type": "code",
        "outputId": "808e1d58-e644-4d4d-fdb9-7663122610b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# If \n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'IKzeNLCo$QHaWfxxQ:;t\\n?SjdMYm go$?pRpppO.nAtXI;Us3ysQy3XOGIys?gXSBTNPcw,obG.f .MySQHnHw3! BgeKHr.-xFl'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afp_jLTgso6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGllY6ypsxmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLKTh6HWs2zN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi4x_VpE0_8Z",
        "colab_type": "code",
        "outputId": "385f2897-7f94-4f20-8df9-8df1bc8c927b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "model.fit(data, epochs=10, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 2.5756\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 1.8796\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 1.6307\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 1.4990\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 1.4203\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 1.3646\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 1.3204\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 1.2820\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 1.2445\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 9s 53ms/step - loss: 1.2091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3d9b5a2208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK2tHfy92HEa",
        "colab_type": "code",
        "outputId": "743dda35-ece9-4312-ffc6-f3373baafe8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "478d_Tbl3Npk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, 1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdWqldgFuRLy",
        "colab_type": "code",
        "outputId": "afa401e7-5896-4a98-80af-e39e7f2f5601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtB1WALauVvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = .65\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5Lf1F0R3tDJ",
        "colab_type": "code",
        "outputId": "715fea41-1588-43cb-ef09-3a25e9df553b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\" \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " here.\n",
            "\n",
            "GREMIO:\n",
            "Who comes it that falls the grace of all.\n",
            "\n",
            "HERMIONE:\n",
            "I take him and so.\n",
            "\n",
            "JULIET:\n",
            "He's standance is set down the city.\n",
            "\n",
            "PETRUCHIO:\n",
            "Come, you are as his force and strange news as in a braverent\n",
            "As I can do it, lords: the other of my husband\n",
            "Which thereof will speak a man; and here is mine and blood.\n",
            "\n",
            "JULIET:\n",
            "It is a good heart him in a husband's land,\n",
            "That fear the sight of comfort but the castle.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Sir, I come to him; there is my never be not might be so,\n",
            "That is't unseen'd with him: he is a sin in in an hand,\n",
            "And something that we had merely in death.\n",
            "\n",
            "GREMIO:\n",
            "Didst thou learn in such a constant death?\n",
            "\n",
            "Second Musician:\n",
            "Farewell, I think, I speak, you have no less\n",
            "Than in his report in the fixe of mine,\n",
            "Who with deceived his design for me.\n",
            "\n",
            "LADY ANNE:\n",
            "And I to see thee might to be constant to come on.\n",
            "\n",
            "GLOUCESTER:\n",
            "I do repent thee or the else that they have no joy\n",
            "That would have given them to the father's life,\n",
            "He perished in a pilth, die.\n",
            "\n",
            "ROMEO:\n",
            "What, S\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}