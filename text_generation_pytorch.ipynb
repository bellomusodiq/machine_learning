{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1heEcbYxe6GselqjgeTYYdbzQnbFJlkxF",
      "authorship_tag": "ABX9TyMAlImz7PSaifIOou+dv/9h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bellomusodiq/machine_learning/blob/master/text_generation_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jir5Rxn8EPmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEf-7XxHFoKK",
        "colab_type": "code",
        "outputId": "b8843e33-2c34-4359-efc2-b208d596d8e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZspY1EGGmnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "  def __init__(self):\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.idx = 0\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2idx:\n",
        "      self.word2idx[word] = self.idx\n",
        "      self.idx2word[self.idx] = word\n",
        "      self.idx += 1\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.word2idx)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWXKNXJ6HLvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextProcess(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.dictionary = Dictionary()\n",
        "\n",
        "  def get_data(self, path, batch_size=20):\n",
        "    vocab = []\n",
        "    with open(path, 'r') as f:\n",
        "      tokens = 0\n",
        "      for line in f:\n",
        "        words = line.split() + ['<eos>']\n",
        "        tokens += len(words)\n",
        "        for word in words:\n",
        "          self.dictionary.add_word(word)\n",
        "          vocab.append(self.dictionary.word2idx[word])\n",
        "    rep_tensor = torch.tensor(vocab, dtype=torch.int64)\n",
        "    num_batches = rep_tensor.shape[0] // batch_size\n",
        "    rep_tensor = rep_tensor[:num_batches*batch_size]\n",
        "    rep_tensor = rep_tensor.view(batch_size, -1)\n",
        "    return rep_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKeA264UJOUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 128\n",
        "hidden_size = 1024\n",
        "num_layers = 1\n",
        "num_epoch = 200\n",
        "batch_size = 20\n",
        "timestep = 30\n",
        "learning_rate = .002"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmNFLyjwUlJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = TextProcess()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g9zpi0gVVok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rep_tensor = corpus.get_data('drive/My Drive/alice.txt', batch_size).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKskc5bqV4nt",
        "colab_type": "code",
        "outputId": "471993c1-c184-4871-aa5e-a6a6c3c0e13f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rep_tensor.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 1484])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JYosyjJWB2Y",
        "colab_type": "code",
        "outputId": "b1aca939-527a-418a-9343-6dff845eaafe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab_size = len(corpus.dictionary)\n",
        "vocab_size"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5290"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3kEQAHZWjdR",
        "colab_type": "code",
        "outputId": "72f8ec4c-b8cc-40ad-df1c-e32ba289c820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_batches = rep_tensor.shape[1] // timestep\n",
        "num_batches"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHRTcFRnYAKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextGenerator(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "    super(TextGenerator, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, x, h):\n",
        "    x = self.embed(x)\n",
        "    out, (h, c) = self.lstm(x, h)\n",
        "    out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "    out = self.linear(out)\n",
        "    return out, (h, c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6Kp7aRjcGwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = TextGenerator(vocab_size, embed_size, hidden_size, num_layers).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1SOnjwPs7mQ",
        "colab_type": "code",
        "outputId": "87e51782-c600-408e-dff0-03e5ec0b55ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TextGenerator(\n",
            "  (embed): Embedding(5290, 128)\n",
            "  (lstm): LSTM(128, 1024, batch_first=True)\n",
            "  (linear): Linear(in_features=1024, out_features=5290, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYrubR3ctAnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JS8SMwktajq",
        "colab_type": "code",
        "outputId": "8208547e-f1f3-4784-ddb3-3f2a67aca28e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(num_epoch):\n",
        "\n",
        "  states = (\n",
        "      torch.zeros(num_layers, batch_size, hidden_size).cuda(),\n",
        "      torch.zeros(num_layers, batch_size, hidden_size).cuda()\n",
        "  )\n",
        "\n",
        "  for i in range(0, rep_tensor.size(1) - timestep, timestep):\n",
        "    inputs = rep_tensor[:, i:i+timestep].cuda()\n",
        "    targets = rep_tensor[:, (i+1):(i+1)+timestep].cuda()\n",
        "\n",
        "    outputs, _ = model(inputs, states)\n",
        "    loss = loss_fn(outputs, targets.reshape(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    clip_grad_norm_(model.parameters(), .5)\n",
        "    optimizer.step()\n",
        "\n",
        "  print('Epoch {}/{}, Loss: {:.4f}'\n",
        "  .format(epoch+1, num_epoch, loss.item()))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200, Loss: 6.4910\n",
            "Epoch 2/200, Loss: 5.6114\n",
            "Epoch 3/200, Loss: 4.9870\n",
            "Epoch 4/200, Loss: 4.4983\n",
            "Epoch 5/200, Loss: 4.0650\n",
            "Epoch 6/200, Loss: 3.4978\n",
            "Epoch 7/200, Loss: 3.0938\n",
            "Epoch 8/200, Loss: 2.6037\n",
            "Epoch 9/200, Loss: 2.2076\n",
            "Epoch 10/200, Loss: 1.8137\n",
            "Epoch 11/200, Loss: 1.5500\n",
            "Epoch 12/200, Loss: 1.1924\n",
            "Epoch 13/200, Loss: 1.0207\n",
            "Epoch 14/200, Loss: 0.6521\n",
            "Epoch 15/200, Loss: 0.4460\n",
            "Epoch 16/200, Loss: 0.2696\n",
            "Epoch 17/200, Loss: 0.1584\n",
            "Epoch 18/200, Loss: 0.0949\n",
            "Epoch 19/200, Loss: 0.0756\n",
            "Epoch 20/200, Loss: 0.0728\n",
            "Epoch 21/200, Loss: 0.0671\n",
            "Epoch 22/200, Loss: 0.0690\n",
            "Epoch 23/200, Loss: 0.0644\n",
            "Epoch 24/200, Loss: 0.0669\n",
            "Epoch 25/200, Loss: 0.0628\n",
            "Epoch 26/200, Loss: 0.0653\n",
            "Epoch 27/200, Loss: 0.0619\n",
            "Epoch 28/200, Loss: 0.0641\n",
            "Epoch 29/200, Loss: 0.0612\n",
            "Epoch 30/200, Loss: 0.0632\n",
            "Epoch 31/200, Loss: 0.0606\n",
            "Epoch 32/200, Loss: 0.0626\n",
            "Epoch 33/200, Loss: 0.0601\n",
            "Epoch 34/200, Loss: 0.0620\n",
            "Epoch 35/200, Loss: 0.0597\n",
            "Epoch 36/200, Loss: 0.0614\n",
            "Epoch 37/200, Loss: 0.0593\n",
            "Epoch 38/200, Loss: 0.0610\n",
            "Epoch 39/200, Loss: 0.0590\n",
            "Epoch 40/200, Loss: 0.0606\n",
            "Epoch 41/200, Loss: 0.0587\n",
            "Epoch 42/200, Loss: 0.0602\n",
            "Epoch 43/200, Loss: 0.0584\n",
            "Epoch 44/200, Loss: 0.0598\n",
            "Epoch 45/200, Loss: 0.0581\n",
            "Epoch 46/200, Loss: 0.0594\n",
            "Epoch 47/200, Loss: 0.0578\n",
            "Epoch 48/200, Loss: 0.0590\n",
            "Epoch 49/200, Loss: 0.0575\n",
            "Epoch 50/200, Loss: 0.0587\n",
            "Epoch 51/200, Loss: 0.0572\n",
            "Epoch 52/200, Loss: 0.0584\n",
            "Epoch 53/200, Loss: 0.0569\n",
            "Epoch 54/200, Loss: 0.0581\n",
            "Epoch 55/200, Loss: 0.0566\n",
            "Epoch 56/200, Loss: 0.0578\n",
            "Epoch 57/200, Loss: 0.0563\n",
            "Epoch 58/200, Loss: 0.0574\n",
            "Epoch 59/200, Loss: 0.0561\n",
            "Epoch 60/200, Loss: 0.0571\n",
            "Epoch 61/200, Loss: 0.0558\n",
            "Epoch 62/200, Loss: 0.0567\n",
            "Epoch 63/200, Loss: 0.0555\n",
            "Epoch 64/200, Loss: 0.0564\n",
            "Epoch 65/200, Loss: 0.0553\n",
            "Epoch 66/200, Loss: 0.0561\n",
            "Epoch 67/200, Loss: 0.0550\n",
            "Epoch 68/200, Loss: 0.0558\n",
            "Epoch 69/200, Loss: 0.0547\n",
            "Epoch 70/200, Loss: 0.0555\n",
            "Epoch 71/200, Loss: 0.0545\n",
            "Epoch 72/200, Loss: 0.0551\n",
            "Epoch 73/200, Loss: 0.0542\n",
            "Epoch 74/200, Loss: 0.0548\n",
            "Epoch 75/200, Loss: 0.0540\n",
            "Epoch 76/200, Loss: 0.0545\n",
            "Epoch 77/200, Loss: 0.0537\n",
            "Epoch 78/200, Loss: 0.0542\n",
            "Epoch 79/200, Loss: 0.0535\n",
            "Epoch 80/200, Loss: 0.0539\n",
            "Epoch 81/200, Loss: 0.0532\n",
            "Epoch 82/200, Loss: 0.0536\n",
            "Epoch 83/200, Loss: 0.0530\n",
            "Epoch 84/200, Loss: 0.0534\n",
            "Epoch 85/200, Loss: 0.0527\n",
            "Epoch 86/200, Loss: 0.0531\n",
            "Epoch 87/200, Loss: 0.0525\n",
            "Epoch 88/200, Loss: 0.0528\n",
            "Epoch 89/200, Loss: 0.0522\n",
            "Epoch 90/200, Loss: 0.0526\n",
            "Epoch 91/200, Loss: 0.0519\n",
            "Epoch 92/200, Loss: 0.0523\n",
            "Epoch 93/200, Loss: 0.0518\n",
            "Epoch 94/200, Loss: 0.0520\n",
            "Epoch 95/200, Loss: 0.0516\n",
            "Epoch 96/200, Loss: 0.0517\n",
            "Epoch 97/200, Loss: 0.0513\n",
            "Epoch 98/200, Loss: 0.0514\n",
            "Epoch 99/200, Loss: 0.0512\n",
            "Epoch 100/200, Loss: 0.0511\n",
            "Epoch 101/200, Loss: 0.0509\n",
            "Epoch 102/200, Loss: 0.0509\n",
            "Epoch 103/200, Loss: 0.0507\n",
            "Epoch 104/200, Loss: 0.0507\n",
            "Epoch 105/200, Loss: 0.0505\n",
            "Epoch 106/200, Loss: 0.0504\n",
            "Epoch 107/200, Loss: 0.0503\n",
            "Epoch 108/200, Loss: 0.0502\n",
            "Epoch 109/200, Loss: 0.0501\n",
            "Epoch 110/200, Loss: 0.0500\n",
            "Epoch 111/200, Loss: 0.0498\n",
            "Epoch 112/200, Loss: 0.0498\n",
            "Epoch 113/200, Loss: 0.0496\n",
            "Epoch 114/200, Loss: 0.0495\n",
            "Epoch 115/200, Loss: 0.0494\n",
            "Epoch 116/200, Loss: 0.0493\n",
            "Epoch 117/200, Loss: 0.0492\n",
            "Epoch 118/200, Loss: 0.0491\n",
            "Epoch 119/200, Loss: 0.0490\n",
            "Epoch 120/200, Loss: 0.0489\n",
            "Epoch 121/200, Loss: 0.0488\n",
            "Epoch 122/200, Loss: 0.0487\n",
            "Epoch 123/200, Loss: 0.0486\n",
            "Epoch 124/200, Loss: 0.0486\n",
            "Epoch 125/200, Loss: 0.0484\n",
            "Epoch 126/200, Loss: 0.0484\n",
            "Epoch 127/200, Loss: 0.0482\n",
            "Epoch 128/200, Loss: 0.0482\n",
            "Epoch 129/200, Loss: 0.0481\n",
            "Epoch 130/200, Loss: 0.0480\n",
            "Epoch 131/200, Loss: 0.0479\n",
            "Epoch 132/200, Loss: 0.0478\n",
            "Epoch 133/200, Loss: 0.0477\n",
            "Epoch 134/200, Loss: 0.0477\n",
            "Epoch 135/200, Loss: 0.0475\n",
            "Epoch 136/200, Loss: 0.0475\n",
            "Epoch 137/200, Loss: 0.0473\n",
            "Epoch 138/200, Loss: 0.0474\n",
            "Epoch 139/200, Loss: 0.0472\n",
            "Epoch 140/200, Loss: 0.0472\n",
            "Epoch 141/200, Loss: 0.0470\n",
            "Epoch 142/200, Loss: 0.0471\n",
            "Epoch 143/200, Loss: 0.0468\n",
            "Epoch 144/200, Loss: 0.0469\n",
            "Epoch 145/200, Loss: 0.0467\n",
            "Epoch 146/200, Loss: 0.0468\n",
            "Epoch 147/200, Loss: 0.0465\n",
            "Epoch 148/200, Loss: 0.0466\n",
            "Epoch 149/200, Loss: 0.0464\n",
            "Epoch 150/200, Loss: 0.0465\n",
            "Epoch 151/200, Loss: 0.0462\n",
            "Epoch 152/200, Loss: 0.0463\n",
            "Epoch 153/200, Loss: 0.0460\n",
            "Epoch 154/200, Loss: 0.0462\n",
            "Epoch 155/200, Loss: 0.0459\n",
            "Epoch 156/200, Loss: 0.0461\n",
            "Epoch 157/200, Loss: 0.0458\n",
            "Epoch 158/200, Loss: 0.0459\n",
            "Epoch 159/200, Loss: 0.0457\n",
            "Epoch 160/200, Loss: 0.0458\n",
            "Epoch 161/200, Loss: 0.0455\n",
            "Epoch 162/200, Loss: 0.0457\n",
            "Epoch 163/200, Loss: 1.3407\n",
            "Epoch 164/200, Loss: 1.0581\n",
            "Epoch 165/200, Loss: 0.5318\n",
            "Epoch 166/200, Loss: 0.3060\n",
            "Epoch 167/200, Loss: 0.1926\n",
            "Epoch 168/200, Loss: 0.1326\n",
            "Epoch 169/200, Loss: 0.1137\n",
            "Epoch 170/200, Loss: 0.0860\n",
            "Epoch 171/200, Loss: 0.0606\n",
            "Epoch 172/200, Loss: 0.0550\n",
            "Epoch 173/200, Loss: 0.0492\n",
            "Epoch 174/200, Loss: 0.0517\n",
            "Epoch 175/200, Loss: 0.0482\n",
            "Epoch 176/200, Loss: 0.0504\n",
            "Epoch 177/200, Loss: 0.0477\n",
            "Epoch 178/200, Loss: 0.0496\n",
            "Epoch 179/200, Loss: 0.0474\n",
            "Epoch 180/200, Loss: 0.0490\n",
            "Epoch 181/200, Loss: 0.0473\n",
            "Epoch 182/200, Loss: 0.0485\n",
            "Epoch 183/200, Loss: 0.0472\n",
            "Epoch 184/200, Loss: 0.0482\n",
            "Epoch 185/200, Loss: 0.0472\n",
            "Epoch 186/200, Loss: 0.0479\n",
            "Epoch 187/200, Loss: 0.0471\n",
            "Epoch 188/200, Loss: 0.0476\n",
            "Epoch 189/200, Loss: 0.0470\n",
            "Epoch 190/200, Loss: 0.0475\n",
            "Epoch 191/200, Loss: 0.0470\n",
            "Epoch 192/200, Loss: 0.0473\n",
            "Epoch 193/200, Loss: 0.0469\n",
            "Epoch 194/200, Loss: 0.0472\n",
            "Epoch 195/200, Loss: 0.0468\n",
            "Epoch 196/200, Loss: 0.0471\n",
            "Epoch 197/200, Loss: 0.0467\n",
            "Epoch 198/200, Loss: 0.0470\n",
            "Epoch 199/200, Loss: 0.0467\n",
            "Epoch 200/200, Loss: 0.0469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM8fq6FU1cap",
        "colab_type": "code",
        "outputId": "66c099a8-91b0-41dc-d996-a5a5ec8fa736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "  with open('results.txt', 'w') as f:\n",
        "    state = (\n",
        "        torch.zeros(num_layers, 1, hidden_size).cuda(),\n",
        "        torch.zeros(num_layers, 1, hidden_size).cuda(),\n",
        "    )\n",
        "    input = torch.randint(0, vocab_size, (1,)).long().unsqueeze(0)\n",
        "    \n",
        "    for i in range(500):\n",
        "      output, _ = model(input.cuda(), state)\n",
        "      prob = output.exp()\n",
        "      word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "      input.fill_(word_id)\n",
        "\n",
        "      word = corpus.dictionary.idx2word[word_id]\n",
        "      word = '\\n' if word == '<eos>' else word + ' '\n",
        "      f.write(word)\n",
        "\n",
        "      if (i+1) % 100 == 0:\n",
        "        print('Sampled {}/{}'.format(i, 500))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sampled 99/500\n",
            "Sampled 199/500\n",
            "Sampled 299/500\n",
            "Sampled 399/500\n",
            "Sampled 499/500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOKbr_Yr321E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "872b838d-691b-4024-d846-c313f7702806"
      },
      "source": [
        "targets.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 30])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}